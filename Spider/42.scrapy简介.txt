scrapy框架：
 -什么是框架？
     就是一个较为通用的项目框架
 -如何学习框架？
     专门学习框架封装的各种功能和用法
 -什么是scrapy框架？
     - 爬虫中封装好的一个明星框架。需要知道封装的功能：高性能的持久化存储，异步下载，高性能数据解析......

-scrapy框架的基本使用：
    -框架的安装：pip install scrapy

    -创建一个工程：scrapy startproject xxxPro(需要先想好创建在那个文件夹，然后切换目录，输入指令)

    -在spiders子目录中创建一个爬虫文件：scrapy genspider spiderName www.xxx.com(执行这段代码,记得要先切换路径，爬虫文件名字自己起，url可以先随便写，后续可以改)

    -在每次执行工程前：改robots协议为False，改UA伪装，加LOG_LEVEL = 'ERROR'

    -执行工程：scrapy crawl spiderName----（LOG_LEVEL = 'ERROR'写到配置文件中可以使不输出那么多日志出来）

    -scrapy数据解析：基本上来讲和前面差不多，但是需要知道的是，在scrapy中不需要在将response转为文本，并且解析完成之后返回的类型是Selector，需要用extract方法将其中的字符串提取出来
                   这个表达式可以将列表转成字符串：page_text = ''.join(page_text)

    -scrapy持久化存储：
        -基于终端指令的~~~:
            -要求：只可以将parse方法的返回值存储到本地的文本文件中
            -注意：持久化存储对应的文本文件类型只可以为：json,jsonlines,jl,csv,xml,marshal,pickle
            -指令：scrapy crawl xxx -o filePath---例如:scrapy crawl qiushibk -o ./Qiusbk.csv
            -优点：简洁高效便捷
            -缺点：局限性比较强（数据只可以存储到指定后缀的文本文件中）
       -基于管道的~~~:
            -编码流程：
                -数据解析
                -在item类中定义相关(要存储数据的那个东西)的属性
                -将解析的数据封装到item类型的对象
                -将item类型的对象提交给管道进行持久化存储
                -在管道类的process_item中要将其接受到的item对象中储存的数进行持久化存储
                -在配置文件中开启管道
                -注意：在按顺序储存的时候要分别打开另外几个文件进行一些代码编写
                -好处：通用性强但是编码流程也太繁琐了
            -如何让一份数据存入本地，一份存到数据库：（未学）
                -管道文件中一个管道类对应的是将数据存储到该平台
                -爬虫文件提交的item只会给管道文件中第一个被执行的管道类所接受
                -process_item中的return item表示将item传递给下一个即将被执行的管道类
            -基于spider的全站数据的爬取：
                -就是将网站中某板块下的全部页码对应的页面数据进行爬取
                -需求：爬取校花网站中校花的名字
